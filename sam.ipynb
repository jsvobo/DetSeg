{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#libs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchmetrics.classification import JaccardIndex\n",
    "\n",
    "\n",
    "#our classes\n",
    "import utils #contains sam_utils, visual_utils, and other utility functions\n",
    "from datasets.dataset_loading import CocoLoader,get_coco_split\n",
    "import segmentation_models\n",
    "\n",
    "#sam\n",
    "from segment_anything import SamPredictor, sam_model_registry, SamAutomaticMaskGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA tests\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # might not be viable, check again!\n",
    "\n",
    "print(\"CUDA available:\" + str(torch.cuda.is_available()))\n",
    "from torch.utils.cpp_extension import CUDA_HOME\n",
    "\n",
    "print(\"CUDA_HOME:\" + str(CUDA_HOME))\n",
    "current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "print(available_gpus)\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = None\n",
    "coco_train_dataset = CocoLoader(get_coco_split(split = 'val'), transform=transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAM wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_wrapper = segmentation_models.SamWrapper(device=\"cuda\", model=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_show = 2\n",
    "offset = 1  # from the beginning of the dataset\n",
    "\n",
    "# loads the wanted images\n",
    "items = coco_train_dataset.get_amount(amount=to_show, offset=offset)\n",
    "\n",
    "results_all = sam_wrapper.infer_masks(items, boxes=None)  # if no promptr, then gt box\n",
    "\n",
    "for i, item in enumerate(items):  # visualise single image and masks on a grid\n",
    "    # inferred results\n",
    "    results = results_all[i]\n",
    "    titles = [\"Score \" + str(sc) for sc in results[\"scores\"]]\n",
    "\n",
    "    # image and GT\n",
    "    img = item[\"image\"]\n",
    "    gt_boxes = item[\"annotations\"][\"boxes\"]\n",
    "\n",
    "    utils.grid_masks_boxes(\n",
    "        image=img, masks=results[\"masks\"], titles=titles, boxes=gt_boxes, scale=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_show = 60  # how many images to show\n",
    "offset = 0  # from the beginning of the dataset\n",
    "\n",
    "show=False\n",
    "IoU_threshold = 0.3\n",
    "\n",
    "# initialise metrics class(es)\n",
    "IoU_metric = JaccardIndex(\"binary\")\n",
    "items = coco_train_dataset.get_amount(amount=to_show, offset=offset)\n",
    "\n",
    "results = sam_wrapper.infer_masks(items)\n",
    "\n",
    "# compare resulting masks with the GT masks, calculate IoU. save those with low IoU\n",
    "bad_masks = []\n",
    "cats = []\n",
    "IoU_all=[]\n",
    "for result, item in zip(results, items): \n",
    "    IoUs_my = utils.get_IoU_multiple(result[\"masks\"], item[\"annotations\"][\"masks\"])\n",
    "    IoU_all.extend(IoUs_my)\n",
    "\n",
    "    for j, (inferred_mask, gt_mask) in enumerate(\n",
    "        zip(result[\"masks\"], item[\"annotations\"][\"masks\"])\n",
    "    ):\n",
    "        IoU_tmp = IoU_metric.forward(inferred_mask, gt_mask)\n",
    "\n",
    "        if IoU_tmp <= IoU_threshold:  # metrics for individual masks\n",
    "            bad_masks.append(\n",
    "                {\n",
    "                    \"image\": item[\"image\"],\n",
    "                    \"box\": item[\"annotations\"][\"boxes\"][j],\n",
    "                    \"inferred_mask\": inferred_mask,\n",
    "                    \"gt_mask\": gt_mask,\n",
    "                }\n",
    "            )\n",
    "            cats.append(item[\"annotations\"][\"categories\"][j])\n",
    "print(\"Mean IoU:\", IoU_metric.compute())\n",
    "print(\"My IoU: \", np.mean(IoU_all))\n",
    "\n",
    "# visualise the bad masks\n",
    "# takes dict with image, box, inferred_mask, gt_mask\n",
    "if show:\n",
    "    cats = coco_train_dataset.translate_catIDs(cats)\n",
    "    for cat, dict_bad_mask in zip(cats, bad_masks):\n",
    "        utils.show_differences(dict_bad_mask, gt_class=cat, segmentation_model=\"SAM-1 (b)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 7  # which image to show\n",
    "\n",
    "item = coco_train_dataset[pos]\n",
    "detected_boxes = item[\"annotations\"][\"boxes\"]  # or detect here\n",
    "\n",
    "# for every box, generate an array points and array of labels\n",
    "middle_points = np.array([[utils.get_middle_point(box)] for box in detected_boxes])\n",
    "point_labels = np.ones((len(detected_boxes), 1), dtype=np.int32)\n",
    "\n",
    "# points: [ [[x,y],[x,y]] ,\n",
    "#          [[x,y],[x,y]]...] , then inside [] for multiple images\n",
    "# point_labels: [ [1],[1] ] , then inside [] for multiple images\n",
    "\n",
    "\n",
    "# infer masks without and with point prompts.\n",
    "results_box = sam_wrapper.infer_masks([item], boxes=[detected_boxes])\n",
    "results_box_middle = sam_wrapper.infer_masks(\n",
    "    [item], boxes=[detected_boxes], points=[middle_points], point_labels=[point_labels]\n",
    ")\n",
    "\n",
    "\n",
    "results_box = results_box[0]\n",
    "results_box_middle = results_box_middle[0]\n",
    "# visuals\n",
    "for i, (point, box) in enumerate(zip(middle_points, detected_boxes)):\n",
    "    # every box multiple times with different masks (+ point in the last)\n",
    "    utils.grid_masks_boxes(\n",
    "        image=item[\"image\"],\n",
    "        masks=[results_box[\"masks\"][i], results_box_middle[\"masks\"][i]],\n",
    "        boxes=[box, box],  # same box\n",
    "        titles=[\"Box\", \"Box + Point\"],\n",
    "        points=[None, point],  # point prompt only in the second one, None otherwise\n",
    "        point_labels=[None, 1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAM automatic mask generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'segmentation_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m conf_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m  { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/vrg2/imdec/models/sam1/sam_vit_b_01ec64.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit_b\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit_h\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 8\u001b[0m _,sam_model \u001b[38;5;241m=\u001b[39m \u001b[43msegmentation_models\u001b[49m\u001b[38;5;241m.\u001b[39mprepare_sam(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m,device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m,cfg \u001b[38;5;241m=\u001b[39m conf_dict) \u001b[38;5;66;03m#load once\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# then reinitialise just mask generator\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# can be used in different models (sam and clip detection?)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'segmentation_models' is not defined"
     ]
    }
   ],
   "source": [
    "conf_dict = {\"b\":\n",
    " { \"path\": \"/mnt/vrg2/imdec/models/sam1/sam_vit_b_01ec64.pth\",\n",
    "  \"model_type\": \"vit_b\"},\n",
    "\"h\":{\n",
    "  \"path\": \"/mnt/vrg2/imdec/models/sam1/sam_vit_h_4b8939.pth\",\n",
    "  \"model_type\": \"vit_h\"}\n",
    "}\n",
    "_,sam_model = segmentation_models.prepare_sam(model='b',device=\"cuda\",cfg = conf_dict) #load once\n",
    "# then reinitialise just mask generator\n",
    "# can be used in different models (sam and clip detection?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_show = 4\n",
    "offset = 7  # from the beginning of the dataset\n",
    "\n",
    "# get images\n",
    "items = coco_train_dataset.get_amount(amount=to_show, offset=offset)\n",
    "\n",
    "\n",
    "# prepare automatic generator\n",
    "mask_generator = segmentation_models.AutomaticSam(\n",
    "    model=sam_model,\n",
    "    points_per_side=32,\n",
    "    pred_iou_thresh=0.9,  # rejects \"bad\" masks\n",
    "    stability_score_thresh=0.95,  # rejects \"bad\" masks\n",
    "    crop_n_layers=1,\n",
    "    crop_n_points_downscale_factor=2,\n",
    "    min_mask_region_area=200,  # Requires open-cv to run post-processing\n",
    ")\n",
    "\n",
    "# segment all automatically using the generator above\n",
    "automatic_segmentations = mask_generator.automatic_multiple_images(\n",
    "    items\n",
    ")\n",
    "\n",
    "# visualise\n",
    "for i, item in enumerate(items):\n",
    "    utils.print_masks_boxes(\n",
    "        image=item[\"image\"], masks=automatic_segmentations[i][\"masks\"], boxes=None\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detect_env_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
